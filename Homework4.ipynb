{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4: Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a deep generative model of binary images (MNIST) using variational autoencoders and generative adversarial networks.\n",
    "The original VAE paper can be found [here](https://arxiv.org/abs/1312.6114) and GANs [here](https://arxiv.org/abs/1406.2661), and there are many excellent tutorials\n",
    "online, e.g. [here](https://arxiv.org/abs/1606.05908) and [here](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
    "\n",
    "**For this homework there will not be a Kaggle submission**\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a discrete deep generative model of binary digits (MNIST) using variational autoencoders\n",
    "2. Examine the learned latent space with visualizations \n",
    "3. Build a continuous deep geneartive model using generative adversarial networks.\n",
    "4. Additionally extend the above in any way, for example by :\n",
    "    - using better encoder/decoders (e.g. CNN as the encoder, PixelCNN as the decoder. Description of PixelCNN \n",
    "    can be found [here](https://arxiv.org/abs/1601.06759))\n",
    "    - using different variational families, e.g. with [normalizing flows](https://arxiv.org/abs/1505.05770), \n",
    "    [inverse autoregressive flows](https://arxiv.org/pdf/1606.04934.pdf), \n",
    "    [hierarchical models](https://arxiv.org/pdf/1602.02282.pdf)\n",
    "    - comparing with stochastic variational inference (i.e. where your variational parameters are randomly initialized and\n",
    "    then updated with gradient ascent on the ELBO\n",
    "    - or your own extension.\n",
    "\n",
    "For your encoder/decoder, we suggest starting off with simple models (e.g. 2-layer MLP with ReLUs).\n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as always, let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1176\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1922  0.9333\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0706  0.8588\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3137\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0902\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0706  0.6706  0.8588\n",
       "  0.0000  0.0000  0.0000  0.0000  0.2157  0.6745  0.8863  0.9922  0.9922\n",
       "  0.0000  0.0000  0.0000  0.0000  0.5333  0.9922  0.9922  0.9922  0.8314\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0118  0.0706  0.0706  0.0706  0.4941  0.5333\n",
       "  0.1412  0.3686  0.6039  0.6667  0.9922  0.9922  0.9922  0.9922  0.9922\n",
       "  0.9922  0.9922  0.9922  0.9922  0.9922  0.9922  0.9922  0.9922  0.9843\n",
       "  0.9922  0.9922  0.9922  0.9922  0.9922  0.7765  0.7137  0.9686  0.9451\n",
       "  0.6118  0.4196  0.9922  0.9922  0.8039  0.0431  0.0000  0.1686  0.6039\n",
       "  0.0549  0.0039  0.6039  0.9922  0.3529  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.5451  0.9922  0.7451  0.0078  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0431  0.7451  0.9922  0.2745  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.1373  0.9451  0.8824  0.6275  0.4235  0.0039\n",
       "  0.0000  0.0000  0.0000  0.0000  0.3176  0.9412  0.9922  0.9922  0.4667\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.1765  0.7294  0.9922  0.9922\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0627  0.3647  0.9882\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.9765\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.1804  0.5098  0.7176  0.9922\n",
       "  0.0000  0.0000  0.0000  0.1529  0.5804  0.8980  0.9922  0.9922  0.9922\n",
       "  0.0000  0.0941  0.4471  0.8667  0.9922  0.9922  0.9922  0.9922  0.7882\n",
       "  0.2588  0.8353  0.9922  0.9922  0.9922  0.9922  0.7765  0.3176  0.0078\n",
       "  0.9922  0.9922  0.9922  0.9922  0.7647  0.3137  0.0353  0.0000  0.0000\n",
       "  0.9922  0.9922  0.9569  0.5216  0.0431  0.0000  0.0000  0.0000  0.0000\n",
       "  0.5294  0.5176  0.0627  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 18 to 26 \n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.6863  0.1020  0.6510  1.0000  0.9686  0.4980  0.0000  0.0000  0.0000\n",
       "  0.8824  0.6745  0.9922  0.9490  0.7647  0.2510  0.0000  0.0000  0.0000\n",
       "  0.3647  0.3216  0.3216  0.2196  0.1529  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0980  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.5882  0.1059  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.9922  0.7333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.9922  0.9765  0.2510  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.9922  0.8118  0.0078  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.9804  0.7137  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.3059  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 27 to 27 \n",
       "   0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "  0.0000\n",
       "[torch.FloatTensor of size 1x28x28]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default MNIST gives grayscale values between [0,1]. Since we are modeling binary images, we have to turn these\n",
    "into binary values, i.e. $\\{0,1\\}^{784}$). A standard way to do this is to interpret the grayscale values as \n",
    "probabilities and sample Bernoulli random vectors based on these probabilities. (Note you should not do this for GANs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 18 \n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   1   0   1   1   1   1   1   1\n",
      "   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   1   1\n",
      "   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   1   0\n",
      "   0   0   0   0   0   0   0   0   1   0   0   1   1   1   0   0   0   1   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   1   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   1   1   1   1   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1   1   1   1   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 19 to 27 \n",
      "    0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   1   1   1   0   0   0   0   0\n",
      "   1   1   1   1   0   0   0   0   0\n",
      "   1   0   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0\n",
      "[torch.FloatTensor of size 1x28x28]\n",
      "\n",
      "torch.Size([60000, 1, 28, 28]) torch.Size([60000]) torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3435)\n",
    "train_img = torch.stack([torch.bernoulli(d[0]) for d in train_dataset])\n",
    "train_label = torch.LongTensor([d[1] for d in train_dataset])\n",
    "test_img = torch.stack([torch.bernoulli(d[0]) for d in test_dataset])\n",
    "test_label = torch.LongTensor([d[1] for d in test_dataset])\n",
    "print(train_img[0])\n",
    "print(train_img.size(), train_label.size(), test_img.size(), test_label.size())\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST does not have an official train dataset. So we will use the last 10000 training points as your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_img = train_img[-10000:].clone()\n",
    "val_label = train_label[-10000:].clone()\n",
    "train_img = train_img[:-10000]\n",
    "train_label = train_label[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the dataloader to split into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(train_img, train_label)\n",
    "val = torch.utils.data.TensorDataset(val_img, val_label)\n",
    "test = torch.utils.data.TensorDataset(test_img, test_label)\n",
    "BATCH_SIZE = 100\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for datum in train_loader:\n",
    "    img, label = datum\n",
    "    print(img.size(), label.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now we are ready to begin modeling. Performance-wise, you want tune your hyperparameters based on the **evidence lower bound (ELBO)**. Recall that the ELBO is given by:\n",
    "\n",
    "$$ELBO = \\mathbb{E}_{q(\\mathbf{z} ; \\lambda)} [\\log p(\\mathbf{x} \\,|\\,\\mathbf{z} ; \\theta)] - \\mathbb{KL}[q(\\mathbf{z};\\lambda) \\, \\Vert \\, p(\\mathbf{z})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational parameters are given by running the encoder over the input, i..e. $\\lambda = encoder(\\mathbf{x};\\phi)$. The generative model (i.e. decoder) is parameterized by $\\theta$. Since we are working with binarized digits, $\\log p(x \\, | \\, \\mathbf{z} ; \\theta)$ is given by:\n",
    "\n",
    "$$ \\log p(x \\, | \\, \\mathbf{z} ; \\theta) = \\sum_{i=1}^{784} x_i \\log \\sigma(\\mathbf{h})_{i} + (1-x_i)\\log (1-\\sigma(\\mathbf{h})_{i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{h}$ is the final layer of the generative model (i.e. 28*28 = 784 dimensionval vector), and $\\sigma(\\cdot)$ is the sigmoid function. \n",
    "\n",
    "For the baseline model in this assignment you will be using a spherical normal prior, i.e. $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The variational family will also be normal, i.e. $q(\\mathbf{z} ; \\lambda) = \\mathcal{N}(\\boldsymbol{\\mu}, \\log \\boldsymbol \\sigma^2)$ (here we will work with normal families with diagonal covariance). The KL-divergence between the variational posterior $q(\\mathbf{z})$ and the prior $p(\\mathbf{z})$ has a closed-form analytic solution, which is available in the original VAE paper referenced above. (If you are using the torch distributions package they will automatically calculate it for you, however you will need to use pytorch 0.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GANs you should use the same data in its continuous form. Here use the same prior, but use a multi-layer network to map to a continous 28x28 output space. Then use a multilayer discriminator to classify. \n",
    "\n",
    "For both models you may also consider trying a deconvolutional network (as in DCGAN) to produce output from the latent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class MLP_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Encoder, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 256)\n",
    "        self.layer2_mean = nn.Linear(256, 32)\n",
    "        self.layer2_logvar = nn.Linear(256, 32)\n",
    "\n",
    "    def encode(self, x):\n",
    "        o1 = nn.RELU(self.layer1(x))\n",
    "        mean, logvar = self.layer2_mean(o1), self.layer2_logvar(o1)\n",
    "        return mean, logvar\n",
    "\n",
    "class MLP_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Decoder, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(32, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 784))\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.layers(z)\n",
    "\n",
    "class MLP_VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_VAE, self).__init__()\n",
    "        self.encoder = MLP_Encoder()\n",
    "        self.decoder = MLP_Decoder()\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return mean.add_(eps.mul(std))\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decode(z), mean, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5b9d5ae04209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP_VAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "model = MLP_VAE()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False)\n",
    "    # Given from Appendix B from VAE paper:\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to quantitative metrics (i.e. ELBO), we are also going to ask you to do some qualitative analysis via visualizations. Please include the following in your report:\n",
    "\n",
    "1. Generate a bunch of digits from your generative model (sample $\\mathbf{z} \\sim p(\\mathbf{z})$, then $\\mathbf{x} \\sim p (\\mathbf{x} \\, | \\, \\mathbf{z} ; \\theta$))\n",
    "2. Sample two random latent vectors $\\mathbf{z}_1, \\mathbf{z}_2 \\sim p(\\mathbf{z})$, then sample from their interpolated values, i.e. $\\mathbf{z} \\sim p (\\mathbf{x} \\, | \\, \\alpha\\mathbf{z}_1 + (1-\\alpha)\\mathbf{z}_2; \\theta$) for $\\alpha = \\{0, 0.2, 0.4, 0.6, 0.8 ,1.0 \\}$.\n",
    "3. Train a VAE with 2 latent dimensions. Make a scatter plot of the variational means, $\\mu_1, \\mu_2$, where the color\n",
    "corresponds to the digit.\n",
    "4. With the same model as in (3), pick a 2d grid around the origin (0,0), e.g. with\n",
    "`np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10)`. For each point in the grid $(z_1, z_2)$, generate\n",
    "$\\mathbf{x}$ and show the corresponding digit in the 2d plot. For an example see [here](http://fastforwardlabs.github.io/blog-images/miriam/tableau.1493x693.png) (the right image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
